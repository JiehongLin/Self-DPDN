setting: supervised


optimizer:
  type : Adam
  lr   : 0.001
  betas: [0.5, 0.999]
  eps  : 0.000001
  weight_decay: 0


lr_scheduler:
  type: CyclicLR
  base_lr: 0.00001
  max_lr: 0.001
  step_size_up: 20000
  step_size_down: 20000
  cycle_momentum: False



bn:
  bn_momentum: 0.9
  bn_decay: 0.5
  decay_step: 4000
  bnm_clip: 0.01

max_epoch : 30
num_mini_batch_per_epoch: 4000

num_category: 6
num_prior: 1024

loss:
  gamma1: 5.0
  gamma2: 1.0
  lambda1: 0.2
  lambda2: 0.02
  beta1: 5.0
  beta2: 1.0

train_dataset:
  img_size: 192
  sample_num: 1024
  shift_range: 0.01

train_dataloader:
  syn_bs: 18
  real_bs: 6
  num_workers: 4
  shuffle: True
  drop_last: True
  pin_memory: False

test:
  img_size: 192
  sample_num: 1024

rd_seed: 1

per_val: 10  # do the evaluation per per_val epochs 
per_write: 50  # write down the info per per_write iterations